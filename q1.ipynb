{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),      # random crops\n",
        "    transforms.RandomHorizontalFlip(),         # random horizontal flip\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize to [-1,1]\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader  = DataLoader(testset,  batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7SC0mFfc6MP",
        "outputId": "78fe2c82-606d-43f4-f90a-b9516b7b23fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, image_size=32, patch_size=4, emb_dim=128, depth=6, heads=8, mlp_dim=256, num_classes=10, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by patch size.\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding using a conv layer\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.patch_embed = nn.Conv2d(3, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Class token and positional embeddings\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n",
        "\n",
        "        # Transformer encoder blocks\n",
        "        self.transformer_blocks = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            block = nn.ModuleDict({\n",
        "                'norm1': nn.LayerNorm(emb_dim),\n",
        "                'attn': nn.MultiheadAttention(embed_dim=emb_dim, num_heads=heads, batch_first=True),\n",
        "                'drop1': nn.Dropout(dropout),\n",
        "                'norm2': nn.LayerNorm(emb_dim),\n",
        "                'mlp': nn.Sequential(\n",
        "                    nn.Linear(emb_dim, mlp_dim),\n",
        "                    nn.GELU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.Linear(mlp_dim, emb_dim),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "            })\n",
        "            self.transformer_blocks.append(block)\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
        "\n",
        "        # Initialize parameters\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        # Patchify and embed\n",
        "        x = self.patch_embed(x)           # shape: (B, emb_dim, H/ps, W/ps)\n",
        "        x = x.flatten(2)                  # shape: (B, emb_dim, num_patches)\n",
        "        x = x.transpose(1, 2)             # shape: (B, num_patches, emb_dim)\n",
        "        # Prepend class token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B,1,emb_dim)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)          # (B, num_patches+1, emb_dim)\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embed\n",
        "        # Transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            # Multi-Head Self-Attention with residual\n",
        "            x = x + block['drop1'](block['attn'](block['norm1'](x), block['norm1'](x), block['norm1'](x))[0])\n",
        "            # MLP with residual\n",
        "            x = x + block['mlp'](block['norm2'](x))\n",
        "        x = self.norm(x)\n",
        "        cls_out = x[:, 0]  # Take the CLS token representation\n",
        "        logits = self.classifier(cls_out)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "model = ViT(image_size=32, patch_size=4, emb_dim=128, depth=6, heads=8, mlp_dim=256, num_classes=10, dropout=0.1).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra5a7L9UdKc-",
        "outputId": "0572c32f-697a-4529-8c7c-7f6d446fdbd2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT(\n",
            "  (patch_embed): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "  (transformer_blocks): ModuleList(\n",
            "    (0-5): 6 x ModuleDict(\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (drop1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (4): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (classifier): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "kch8j7lSdZOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function, optimizer, and LR scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)  # cosine decay over 20 epochs\n",
        "\n",
        "num_epochs = 100\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    acc = 100 * correct / total\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_config = {\n",
        "            'patch_size': 4, 'depth': 6, 'emb_dim': 128, 'heads': 8,\n",
        "            'mlp_dim': 256, 'lr': 1e-3, 'weight_decay': 5e-4\n",
        "        }\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "print(\"Best Config:\", best_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zASB9KghdShD",
        "outputId": "358cb8fc-f9eb-4261-9c61-43045df3d524"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.9927, Test Accuracy: 64.41%\n",
            "Epoch [2/100], Loss: 0.9856, Test Accuracy: 66.51%\n",
            "Epoch [3/100], Loss: 0.9650, Test Accuracy: 66.11%\n",
            "Epoch [4/100], Loss: 0.9483, Test Accuracy: 67.08%\n",
            "Epoch [5/100], Loss: 0.9281, Test Accuracy: 67.59%\n",
            "Epoch [6/100], Loss: 0.9062, Test Accuracy: 67.70%\n",
            "Epoch [7/100], Loss: 0.8849, Test Accuracy: 67.72%\n",
            "Epoch [8/100], Loss: 0.8590, Test Accuracy: 69.90%\n",
            "Epoch [9/100], Loss: 0.8373, Test Accuracy: 69.72%\n",
            "Epoch [10/100], Loss: 0.8135, Test Accuracy: 71.35%\n",
            "Epoch [11/100], Loss: 0.7886, Test Accuracy: 71.69%\n",
            "Epoch [12/100], Loss: 0.7568, Test Accuracy: 72.00%\n",
            "Epoch [13/100], Loss: 0.7327, Test Accuracy: 72.58%\n",
            "Epoch [14/100], Loss: 0.7060, Test Accuracy: 73.11%\n",
            "Epoch [15/100], Loss: 0.6867, Test Accuracy: 73.40%\n",
            "Epoch [16/100], Loss: 0.6638, Test Accuracy: 73.92%\n",
            "Epoch [17/100], Loss: 0.6482, Test Accuracy: 73.89%\n",
            "Epoch [18/100], Loss: 0.6313, Test Accuracy: 73.97%\n",
            "Epoch [19/100], Loss: 0.6243, Test Accuracy: 74.29%\n",
            "Epoch [20/100], Loss: 0.6165, Test Accuracy: 74.27%\n",
            "Epoch [21/100], Loss: 0.6147, Test Accuracy: 74.27%\n",
            "Epoch [22/100], Loss: 0.6161, Test Accuracy: 74.34%\n",
            "Epoch [23/100], Loss: 0.6235, Test Accuracy: 74.25%\n",
            "Epoch [24/100], Loss: 0.6252, Test Accuracy: 74.24%\n",
            "Epoch [25/100], Loss: 0.6272, Test Accuracy: 74.33%\n",
            "Epoch [26/100], Loss: 0.6361, Test Accuracy: 74.06%\n",
            "Epoch [27/100], Loss: 0.6439, Test Accuracy: 74.43%\n",
            "Epoch [28/100], Loss: 0.6552, Test Accuracy: 73.34%\n",
            "Epoch [29/100], Loss: 0.6586, Test Accuracy: 73.33%\n",
            "Epoch [30/100], Loss: 0.6715, Test Accuracy: 73.58%\n",
            "Epoch [31/100], Loss: 0.6854, Test Accuracy: 73.18%\n",
            "Epoch [32/100], Loss: 0.6923, Test Accuracy: 72.01%\n",
            "Epoch [33/100], Loss: 0.7035, Test Accuracy: 72.48%\n",
            "Epoch [34/100], Loss: 0.7138, Test Accuracy: 72.22%\n",
            "Epoch [35/100], Loss: 0.7220, Test Accuracy: 71.44%\n",
            "Epoch [36/100], Loss: 0.7282, Test Accuracy: 72.28%\n",
            "Epoch [37/100], Loss: 0.7237, Test Accuracy: 71.98%\n",
            "Epoch [38/100], Loss: 0.7289, Test Accuracy: 72.56%\n",
            "Epoch [39/100], Loss: 0.7232, Test Accuracy: 72.33%\n",
            "Epoch [40/100], Loss: 0.7144, Test Accuracy: 72.65%\n",
            "Epoch [41/100], Loss: 0.7158, Test Accuracy: 70.15%\n",
            "Epoch [42/100], Loss: 0.7073, Test Accuracy: 71.45%\n",
            "Epoch [43/100], Loss: 0.7009, Test Accuracy: 72.50%\n",
            "Epoch [44/100], Loss: 0.6795, Test Accuracy: 72.49%\n",
            "Epoch [45/100], Loss: 0.6632, Test Accuracy: 73.25%\n",
            "Epoch [46/100], Loss: 0.6461, Test Accuracy: 73.36%\n",
            "Epoch [47/100], Loss: 0.6309, Test Accuracy: 73.61%\n",
            "Epoch [48/100], Loss: 0.6060, Test Accuracy: 74.44%\n",
            "Epoch [49/100], Loss: 0.5862, Test Accuracy: 74.06%\n",
            "Epoch [50/100], Loss: 0.5590, Test Accuracy: 74.48%\n",
            "Epoch [51/100], Loss: 0.5349, Test Accuracy: 75.10%\n",
            "Epoch [52/100], Loss: 0.5120, Test Accuracy: 75.02%\n",
            "Epoch [53/100], Loss: 0.4844, Test Accuracy: 75.01%\n",
            "Epoch [54/100], Loss: 0.4588, Test Accuracy: 75.32%\n",
            "Epoch [55/100], Loss: 0.4396, Test Accuracy: 75.89%\n",
            "Epoch [56/100], Loss: 0.4210, Test Accuracy: 76.09%\n",
            "Epoch [57/100], Loss: 0.4067, Test Accuracy: 76.29%\n",
            "Epoch [58/100], Loss: 0.3950, Test Accuracy: 76.55%\n",
            "Epoch [59/100], Loss: 0.3850, Test Accuracy: 76.63%\n",
            "Epoch [60/100], Loss: 0.3791, Test Accuracy: 76.55%\n",
            "Epoch [61/100], Loss: 0.3803, Test Accuracy: 76.55%\n",
            "Epoch [62/100], Loss: 0.3776, Test Accuracy: 76.63%\n",
            "Epoch [63/100], Loss: 0.3804, Test Accuracy: 76.55%\n",
            "Epoch [64/100], Loss: 0.3880, Test Accuracy: 76.36%\n",
            "Epoch [65/100], Loss: 0.3847, Test Accuracy: 76.44%\n",
            "Epoch [66/100], Loss: 0.3920, Test Accuracy: 76.41%\n",
            "Epoch [67/100], Loss: 0.4019, Test Accuracy: 75.75%\n",
            "Epoch [68/100], Loss: 0.4124, Test Accuracy: 75.86%\n",
            "Epoch [69/100], Loss: 0.4287, Test Accuracy: 75.75%\n",
            "Epoch [70/100], Loss: 0.4469, Test Accuracy: 74.86%\n",
            "Epoch [71/100], Loss: 0.4580, Test Accuracy: 74.45%\n",
            "Epoch [72/100], Loss: 0.4699, Test Accuracy: 74.79%\n",
            "Epoch [73/100], Loss: 0.4856, Test Accuracy: 74.44%\n",
            "Epoch [74/100], Loss: 0.5026, Test Accuracy: 75.06%\n",
            "Epoch [75/100], Loss: 0.5058, Test Accuracy: 74.62%\n",
            "Epoch [76/100], Loss: 0.5182, Test Accuracy: 73.90%\n",
            "Epoch [77/100], Loss: 0.5221, Test Accuracy: 75.34%\n",
            "Epoch [78/100], Loss: 0.5258, Test Accuracy: 73.71%\n",
            "Epoch [79/100], Loss: 0.5258, Test Accuracy: 73.15%\n",
            "Epoch [80/100], Loss: 0.5279, Test Accuracy: 74.68%\n",
            "Epoch [81/100], Loss: 0.5267, Test Accuracy: 74.87%\n",
            "Epoch [82/100], Loss: 0.5198, Test Accuracy: 74.93%\n",
            "Epoch [83/100], Loss: 0.5179, Test Accuracy: 74.31%\n",
            "Epoch [84/100], Loss: 0.5037, Test Accuracy: 74.74%\n",
            "Epoch [85/100], Loss: 0.4883, Test Accuracy: 74.80%\n",
            "Epoch [86/100], Loss: 0.4737, Test Accuracy: 74.82%\n",
            "Epoch [87/100], Loss: 0.4514, Test Accuracy: 75.05%\n",
            "Epoch [88/100], Loss: 0.4359, Test Accuracy: 75.49%\n",
            "Epoch [89/100], Loss: 0.4139, Test Accuracy: 75.31%\n",
            "Epoch [90/100], Loss: 0.3873, Test Accuracy: 75.72%\n",
            "Epoch [91/100], Loss: 0.3707, Test Accuracy: 75.19%\n",
            "Epoch [92/100], Loss: 0.3395, Test Accuracy: 76.11%\n",
            "Epoch [93/100], Loss: 0.3234, Test Accuracy: 76.33%\n",
            "Epoch [94/100], Loss: 0.2976, Test Accuracy: 76.09%\n",
            "Epoch [95/100], Loss: 0.2800, Test Accuracy: 76.75%\n",
            "Epoch [96/100], Loss: 0.2617, Test Accuracy: 76.63%\n",
            "Epoch [97/100], Loss: 0.2490, Test Accuracy: 76.38%\n",
            "Epoch [98/100], Loss: 0.2372, Test Accuracy: 76.75%\n",
            "Epoch [99/100], Loss: 0.2290, Test Accuracy: 76.78%\n",
            "Epoch [100/100], Loss: 0.2252, Test Accuracy: 76.67%\n",
            "Best Test Accuracy: 76.78%\n",
            "Best Config: {'patch_size': 4, 'depth': 6, 'emb_dim': 128, 'heads': 8, 'mlp_dim': 256, 'lr': 0.001, 'weight_decay': 0.0005}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on test set\n",
        "model.eval()\n",
        "correct = 0; total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "final_acc = 100 * correct / total\n",
        "print(f\"Final Test Accuracy: {final_acc:.2f}%\")\n",
        "print(\"Best Config:\", best_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7VrudzDdd0w",
        "outputId": "7aaccdc8-4259-4157-a7c2-467c6966e77b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 76.67%\n",
            "Best Config: {'patch_size': 4, 'depth': 6, 'emb_dim': 128, 'heads': 8, 'mlp_dim': 256, 'lr': 0.001, 'weight_decay': 0.0005}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm"
      ],
      "metadata": {
        "id": "n1z5onup1OVT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math, time, os, copy, random\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UPHF3GB1NOA",
        "outputId": "23e0166e-0bce-4c08-a3d6-9de7bc806d23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions: MixUp, CutMix, accuracy\n",
        "def accuracy(output, target):\n",
        "    preds = output.argmax(dim=1)\n",
        "    return (preds == target).float().mean().item()\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha <= 0:\n",
        "        return x, y, None, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    if alpha <= 0:\n",
        "        return x, y, None, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size, _, H, W = x.size()\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    cut_w = int(W * math.sqrt(1 - lam))\n",
        "    cut_h = int(H * math.sqrt(1 - lam))\n",
        "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
        "    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
        "    y_a, y_b = y, y[index]\n",
        "    return x, y_a, y_b, lam\n"
      ],
      "metadata": {
        "id": "CJ4g9tiK1SL2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DropPath (stochastic depth) implementation\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n"
      ],
      "metadata": {
        "id": "S3CXXCSH1XgF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT model (from scratch) with configurable stochastic depth & layer scale\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=128):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # B x D x H' x W'\n",
        "        x = x.flatten(2).transpose(1,2)  # B x N x D\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x); x = self.act(x); x = self.drop(x)\n",
        "        x = self.fc2(x); x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(dim, num_heads=num_heads, batch_first=True, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path>0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, hidden_features=dim*4, drop=proj_drop)\n",
        "        # layer scaling (small init)\n",
        "        self.gamma_1 = nn.Parameter(1e-2 * torch.ones(dim))  # layer scale\n",
        "        self.gamma_2 = nn.Parameter(1e-2 * torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
        "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
        "                 embed_dim=192, depth=12, num_heads=3, mlp_ratio=4., drop_path_rate=0.1, attn_drop=0., drop=0.):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop)\n",
        "        # stochastic depth linear decay\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            AttentionBlock(embed_dim, num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop, drop_path=dpr[i])\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        # init\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:,0])\n"
      ],
      "metadata": {
        "id": "7s7uW_Hp1cSa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data and augmentations (strong)\n",
        "from torchvision.transforms import RandAugment\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2470, 0.2435, 0.2616)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    RandAugment(num_ops=2, magnitude=9),  # strong augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transforms)\n",
        "test_set  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transforms)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHrxnFVe1lWT",
        "outputId": "136298f7-821e-4173-9356-6836f05929a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 40.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training utilities: label smoothing loss, warmup cosine scheduler, EMA\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, pred, target):\n",
        "        log_probs = F.log_softmax(pred, dim=1)\n",
        "        n_classes = pred.size(1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
        "\n",
        "class CosineWarmupScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_epochs, max_epochs, last_epoch=-1):\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.max_epochs = max_epochs\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "    def get_lr(self):\n",
        "        cur = self.last_epoch\n",
        "        if cur < self.warmup_epochs:\n",
        "            return [base_lr * float(cur + 1) / float(self.warmup_epochs) for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            # cosine decay\n",
        "            t = (cur - self.warmup_epochs) / max(1, self.max_epochs - self.warmup_epochs)\n",
        "            return [base_lr * 0.5 * (1 + math.cos(math.pi * t)) for base_lr in self.base_lrs]\n",
        "\n",
        "# Simple EMA\n",
        "class ModelEMA:\n",
        "    def __init__(self, model, decay=0.9999):\n",
        "        self.ema = copy.deepcopy(model).eval()\n",
        "        self.decay = decay\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "    def update(self, model):\n",
        "        with torch.no_grad():\n",
        "            msd = model.state_dict()\n",
        "            for k, v in self.ema.state_dict().items():\n",
        "                if v.dtype.is_floating_point:\n",
        "                    v *= self.decay\n",
        "                    v += (1.0 - self.decay) * msd[k].detach()\n"
      ],
      "metadata": {
        "id": "KpQ_7CYN1rUP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate model, optimizer, scheduler, loss\n",
        "\n",
        "CFG = {\n",
        "    'img_size': 32, 'patch_size': 4, 'embed_dim': 192, 'depth': 12,\n",
        "    'num_heads': 3, 'mlp_ratio': 4.0, 'drop_path_rate': 0.1,\n",
        "    'drop': 0.0, 'attn_drop': 0.0\n",
        "}\n",
        "model = ViT(img_size=CFG['img_size'], patch_size=CFG['patch_size'], embed_dim=CFG['embed_dim'],\n",
        "            depth=CFG['depth'], num_heads=CFG['num_heads'], drop_path_rate=CFG['drop_path_rate']).to(device)\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "base_lr = 3e-3\n",
        "optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.05)\n",
        "NUM_EPOCHS = 120\n",
        "WARMUP_EPOCHS = 5\n",
        "scheduler = CosineWarmupScheduler(optimizer, warmup_epochs=WARMUP_EPOCHS, max_epochs=NUM_EPOCHS)\n",
        "scaler = GradScaler()\n",
        "ema = ModelEMA(model, decay=0.9998)\n",
        "print(\"Total params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HERIZ3E1ttB",
        "outputId": "9d52f8e3-ba66-4901-8d93-2a8d6ecdefa6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params (M): 5.36737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1960274121.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop with MixUp/CutMix + eval\n",
        "use_mixup = True\n",
        "mixup_alpha = 0.8\n",
        "use_cutmix = False   # choose one or both — I've seen MixUp + CutMix both work; set True to try CutMix.\n",
        "\n",
        "best_acc = 0.0\n",
        "save_path = \"best_vit_cifar.pth\"\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n = 0\n",
        "    t0 = time.time()\n",
        "    for images, targets in train_loader:\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # augment via mixup/cutmix\n",
        "        if use_cutmix:\n",
        "            images, y_a, y_b, lam = cutmix_data(images, targets, alpha=mixup_alpha)\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "            scaler.scale(loss).backward()\n",
        "        elif use_mixup:\n",
        "            images, y_a, y_b, lam = mixup_data(images, targets, alpha=mixup_alpha)\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        n += images.size(0)\n",
        "        # update EMA\n",
        "        ema.update(model)\n",
        "    scheduler.step()\n",
        "    # eval\n",
        "    model.eval()\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    acc = 100 * correct / total\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        # save model + ema\n",
        "        torch.save({'model': model.state_dict(), 'ema': ema.ema.state_dict(), 'cfg': CFG}, save_path)\n",
        "    print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS}  Loss: {running_loss/n:.4f}  Test Acc: {acc:.2f}%  Best: {best_acc:.2f}%  LR: {optimizer.param_groups[0]['lr']:.2e}  Time: {(time.time()-t0):.1f}s\")\n",
        "\n",
        "print(\"Training finished. Best test acc:\", best_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UETnTwhh17XA",
        "outputId": "75c5f294-8915-4f14-9242-df45ce3df1d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2657767503.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001/120  Loss: 2.1053  Test Acc: 45.80%  Best: 45.80%  LR: 1.20e-03  Time: 61.3s\n",
            "Epoch 002/120  Loss: 1.9824  Test Acc: 50.00%  Best: 50.00%  LR: 1.80e-03  Time: 56.9s\n",
            "Epoch 003/120  Loss: 1.9556  Test Acc: 52.23%  Best: 52.23%  LR: 2.40e-03  Time: 56.5s\n",
            "Epoch 004/120  Loss: 1.9614  Test Acc: 50.91%  Best: 52.23%  LR: 3.00e-03  Time: 56.7s\n",
            "Epoch 005/120  Loss: 1.9535  Test Acc: 52.59%  Best: 52.59%  LR: 3.00e-03  Time: 56.8s\n",
            "Epoch 006/120  Loss: 1.9343  Test Acc: 51.84%  Best: 52.59%  LR: 3.00e-03  Time: 57.2s\n",
            "Epoch 007/120  Loss: 1.9030  Test Acc: 54.97%  Best: 54.97%  LR: 3.00e-03  Time: 58.2s\n",
            "Epoch 008/120  Loss: 1.8845  Test Acc: 56.91%  Best: 56.91%  LR: 2.99e-03  Time: 57.2s\n",
            "Epoch 009/120  Loss: 1.8630  Test Acc: 58.30%  Best: 58.30%  LR: 2.99e-03  Time: 56.5s\n",
            "Epoch 010/120  Loss: 1.8553  Test Acc: 57.67%  Best: 58.30%  LR: 2.99e-03  Time: 57.0s\n",
            "Epoch 011/120  Loss: 1.8317  Test Acc: 60.23%  Best: 60.23%  LR: 2.98e-03  Time: 57.0s\n",
            "Epoch 012/120  Loss: 1.8112  Test Acc: 63.32%  Best: 63.32%  LR: 2.97e-03  Time: 58.5s\n",
            "Epoch 013/120  Loss: 1.7989  Test Acc: 62.56%  Best: 63.32%  LR: 2.96e-03  Time: 56.4s\n",
            "Epoch 014/120  Loss: 1.7952  Test Acc: 63.25%  Best: 63.32%  LR: 2.95e-03  Time: 56.9s\n",
            "Epoch 015/120  Loss: 1.7641  Test Acc: 63.32%  Best: 63.32%  LR: 2.94e-03  Time: 57.8s\n",
            "Epoch 016/120  Loss: 1.7739  Test Acc: 67.35%  Best: 67.35%  LR: 2.93e-03  Time: 57.0s\n",
            "Epoch 017/120  Loss: 1.7442  Test Acc: 66.68%  Best: 67.35%  LR: 2.92e-03  Time: 56.5s\n",
            "Epoch 018/120  Loss: 1.7561  Test Acc: 67.78%  Best: 67.78%  LR: 2.91e-03  Time: 56.7s\n",
            "Epoch 019/120  Loss: 1.7395  Test Acc: 66.52%  Best: 67.78%  LR: 2.89e-03  Time: 56.2s\n",
            "Epoch 020/120  Loss: 1.7399  Test Acc: 69.21%  Best: 69.21%  LR: 2.88e-03  Time: 56.5s\n",
            "Epoch 021/120  Loss: 1.7134  Test Acc: 69.25%  Best: 69.25%  LR: 2.86e-03  Time: 56.4s\n",
            "Epoch 022/120  Loss: 1.7036  Test Acc: 69.31%  Best: 69.31%  LR: 2.84e-03  Time: 56.3s\n",
            "Epoch 023/120  Loss: 1.7217  Test Acc: 69.46%  Best: 69.46%  LR: 2.82e-03  Time: 56.6s\n",
            "Epoch 024/120  Loss: 1.6944  Test Acc: 69.99%  Best: 69.99%  LR: 2.80e-03  Time: 56.5s\n",
            "Epoch 025/120  Loss: 1.6755  Test Acc: 71.75%  Best: 71.75%  LR: 2.78e-03  Time: 56.6s\n",
            "Epoch 026/120  Loss: 1.6731  Test Acc: 72.43%  Best: 72.43%  LR: 2.76e-03  Time: 57.0s\n",
            "Epoch 027/120  Loss: 1.6835  Test Acc: 73.43%  Best: 73.43%  LR: 2.74e-03  Time: 57.5s\n",
            "Epoch 028/120  Loss: 1.6698  Test Acc: 73.35%  Best: 73.43%  LR: 2.71e-03  Time: 57.2s\n",
            "Epoch 029/120  Loss: 1.6739  Test Acc: 73.96%  Best: 73.96%  LR: 2.69e-03  Time: 56.8s\n",
            "Epoch 030/120  Loss: 1.6477  Test Acc: 74.52%  Best: 74.52%  LR: 2.66e-03  Time: 56.6s\n",
            "Epoch 031/120  Loss: 1.6521  Test Acc: 73.89%  Best: 74.52%  LR: 2.64e-03  Time: 56.2s\n",
            "Epoch 032/120  Loss: 1.6366  Test Acc: 74.27%  Best: 74.52%  LR: 2.61e-03  Time: 56.0s\n",
            "Epoch 033/120  Loss: 1.6381  Test Acc: 75.17%  Best: 75.17%  LR: 2.58e-03  Time: 56.9s\n",
            "Epoch 034/120  Loss: 1.6459  Test Acc: 75.21%  Best: 75.21%  LR: 2.55e-03  Time: 55.9s\n",
            "Epoch 035/120  Loss: 1.6449  Test Acc: 75.50%  Best: 75.50%  LR: 2.52e-03  Time: 56.9s\n",
            "Epoch 036/120  Loss: 1.6206  Test Acc: 75.21%  Best: 75.50%  LR: 2.49e-03  Time: 56.4s\n",
            "Epoch 037/120  Loss: 1.6050  Test Acc: 76.62%  Best: 76.62%  LR: 2.46e-03  Time: 56.5s\n",
            "Epoch 038/120  Loss: 1.6089  Test Acc: 77.13%  Best: 77.13%  LR: 2.43e-03  Time: 57.5s\n",
            "Epoch 039/120  Loss: 1.6084  Test Acc: 75.72%  Best: 77.13%  LR: 2.40e-03  Time: 61.0s\n",
            "Epoch 040/120  Loss: 1.6105  Test Acc: 77.40%  Best: 77.40%  LR: 2.37e-03  Time: 63.8s\n",
            "Epoch 041/120  Loss: 1.5870  Test Acc: 75.56%  Best: 77.40%  LR: 2.33e-03  Time: 61.8s\n",
            "Epoch 042/120  Loss: 1.6061  Test Acc: 75.47%  Best: 77.40%  LR: 2.30e-03  Time: 62.4s\n",
            "Epoch 043/120  Loss: 1.6103  Test Acc: 78.49%  Best: 78.49%  LR: 2.26e-03  Time: 62.3s\n",
            "Epoch 044/120  Loss: 1.5740  Test Acc: 77.72%  Best: 78.49%  LR: 2.23e-03  Time: 63.2s\n",
            "Epoch 045/120  Loss: 1.5726  Test Acc: 77.73%  Best: 78.49%  LR: 2.19e-03  Time: 61.3s\n",
            "Epoch 046/120  Loss: 1.5612  Test Acc: 78.94%  Best: 78.94%  LR: 2.15e-03  Time: 62.5s\n",
            "Epoch 047/120  Loss: 1.5877  Test Acc: 79.21%  Best: 79.21%  LR: 2.12e-03  Time: 63.0s\n",
            "Epoch 048/120  Loss: 1.6038  Test Acc: 78.99%  Best: 79.21%  LR: 2.08e-03  Time: 61.8s\n",
            "Epoch 049/120  Loss: 1.5547  Test Acc: 79.47%  Best: 79.47%  LR: 2.04e-03  Time: 61.7s\n",
            "Epoch 050/120  Loss: 1.5588  Test Acc: 79.28%  Best: 79.47%  LR: 2.00e-03  Time: 61.7s\n",
            "Epoch 051/120  Loss: 1.5593  Test Acc: 79.21%  Best: 79.47%  LR: 1.96e-03  Time: 63.4s\n",
            "Epoch 052/120  Loss: 1.5406  Test Acc: 80.94%  Best: 80.94%  LR: 1.92e-03  Time: 62.1s\n",
            "Epoch 053/120  Loss: 1.5640  Test Acc: 80.43%  Best: 80.94%  LR: 1.88e-03  Time: 61.5s\n",
            "Epoch 054/120  Loss: 1.5473  Test Acc: 80.80%  Best: 80.94%  LR: 1.85e-03  Time: 60.7s\n",
            "Epoch 055/120  Loss: 1.5574  Test Acc: 78.33%  Best: 80.94%  LR: 1.81e-03  Time: 63.5s\n",
            "Epoch 056/120  Loss: 1.5769  Test Acc: 81.93%  Best: 81.93%  LR: 1.76e-03  Time: 62.9s\n",
            "Epoch 057/120  Loss: 1.5696  Test Acc: 81.99%  Best: 81.99%  LR: 1.72e-03  Time: 62.4s\n",
            "Epoch 058/120  Loss: 1.5117  Test Acc: 81.73%  Best: 81.99%  LR: 1.68e-03  Time: 62.9s\n",
            "Epoch 059/120  Loss: 1.5368  Test Acc: 82.17%  Best: 82.17%  LR: 1.64e-03  Time: 63.4s\n",
            "Epoch 060/120  Loss: 1.5152  Test Acc: 81.11%  Best: 82.17%  LR: 1.60e-03  Time: 64.8s\n",
            "Epoch 061/120  Loss: 1.5311  Test Acc: 81.52%  Best: 82.17%  LR: 1.56e-03  Time: 62.2s\n",
            "Epoch 062/120  Loss: 1.5289  Test Acc: 81.96%  Best: 82.17%  LR: 1.52e-03  Time: 63.0s\n",
            "Epoch 063/120  Loss: 1.5399  Test Acc: 82.59%  Best: 82.59%  LR: 1.48e-03  Time: 62.6s\n",
            "Epoch 064/120  Loss: 1.5104  Test Acc: 82.86%  Best: 82.86%  LR: 1.44e-03  Time: 62.9s\n",
            "Epoch 065/120  Loss: 1.5009  Test Acc: 84.04%  Best: 84.04%  LR: 1.40e-03  Time: 63.0s\n",
            "Epoch 066/120  Loss: 1.4829  Test Acc: 83.78%  Best: 84.04%  LR: 1.36e-03  Time: 62.7s\n",
            "Epoch 067/120  Loss: 1.4615  Test Acc: 83.04%  Best: 84.04%  LR: 1.32e-03  Time: 62.6s\n",
            "Epoch 068/120  Loss: 1.4890  Test Acc: 83.88%  Best: 84.04%  LR: 1.28e-03  Time: 62.1s\n",
            "Epoch 069/120  Loss: 1.4703  Test Acc: 84.63%  Best: 84.63%  LR: 1.24e-03  Time: 63.3s\n",
            "Epoch 070/120  Loss: 1.4849  Test Acc: 85.03%  Best: 85.03%  LR: 1.19e-03  Time: 63.4s\n",
            "Epoch 071/120  Loss: 1.4814  Test Acc: 84.56%  Best: 85.03%  LR: 1.15e-03  Time: 63.6s\n",
            "Epoch 072/120  Loss: 1.4341  Test Acc: 84.85%  Best: 85.03%  LR: 1.12e-03  Time: 62.2s\n",
            "Epoch 073/120  Loss: 1.4793  Test Acc: 84.73%  Best: 85.03%  LR: 1.08e-03  Time: 61.8s\n",
            "Epoch 074/120  Loss: 1.4572  Test Acc: 85.41%  Best: 85.41%  LR: 1.04e-03  Time: 62.7s\n",
            "Epoch 075/120  Loss: 1.4339  Test Acc: 84.37%  Best: 85.41%  LR: 9.98e-04  Time: 63.4s\n",
            "Epoch 076/120  Loss: 1.4497  Test Acc: 85.95%  Best: 85.95%  LR: 9.59e-04  Time: 62.3s\n",
            "Epoch 077/120  Loss: 1.4658  Test Acc: 86.04%  Best: 86.04%  LR: 9.21e-04  Time: 61.6s\n",
            "Epoch 078/120  Loss: 1.4561  Test Acc: 85.71%  Best: 86.04%  LR: 8.84e-04  Time: 61.8s\n",
            "Epoch 079/120  Loss: 1.4377  Test Acc: 86.08%  Best: 86.08%  LR: 8.47e-04  Time: 63.2s\n",
            "Epoch 080/120  Loss: 1.4337  Test Acc: 86.93%  Best: 86.93%  LR: 8.10e-04  Time: 61.6s\n",
            "Epoch 081/120  Loss: 1.4290  Test Acc: 86.95%  Best: 86.95%  LR: 7.74e-04  Time: 61.4s\n",
            "Epoch 082/120  Loss: 1.4354  Test Acc: 86.89%  Best: 86.95%  LR: 7.38e-04  Time: 62.0s\n",
            "Epoch 083/120  Loss: 1.4494  Test Acc: 87.15%  Best: 87.15%  LR: 7.03e-04  Time: 62.3s\n",
            "Epoch 084/120  Loss: 1.4363  Test Acc: 88.07%  Best: 88.07%  LR: 6.69e-04  Time: 63.3s\n",
            "Epoch 085/120  Loss: 1.3985  Test Acc: 87.58%  Best: 88.07%  LR: 6.35e-04  Time: 61.9s\n",
            "Epoch 086/120  Loss: 1.3934  Test Acc: 87.64%  Best: 88.07%  LR: 6.02e-04  Time: 62.4s\n",
            "Epoch 087/120  Loss: 1.3873  Test Acc: 88.35%  Best: 88.35%  LR: 5.69e-04  Time: 63.1s\n",
            "Epoch 088/120  Loss: 1.3960  Test Acc: 88.00%  Best: 88.35%  LR: 5.38e-04  Time: 63.5s\n",
            "Epoch 089/120  Loss: 1.3830  Test Acc: 88.21%  Best: 88.35%  LR: 5.06e-04  Time: 62.0s\n",
            "Epoch 090/120  Loss: 1.3825  Test Acc: 88.44%  Best: 88.44%  LR: 4.76e-04  Time: 61.6s\n",
            "Epoch 091/120  Loss: 1.3733  Test Acc: 88.74%  Best: 88.74%  LR: 4.47e-04  Time: 62.6s\n",
            "Epoch 092/120  Loss: 1.3707  Test Acc: 89.15%  Best: 89.15%  LR: 4.18e-04  Time: 63.2s\n",
            "Epoch 093/120  Loss: 1.4114  Test Acc: 89.29%  Best: 89.29%  LR: 3.90e-04  Time: 61.9s\n",
            "Epoch 094/120  Loss: 1.3838  Test Acc: 88.84%  Best: 89.29%  LR: 3.63e-04  Time: 62.0s\n",
            "Epoch 095/120  Loss: 1.3717  Test Acc: 89.48%  Best: 89.48%  LR: 3.36e-04  Time: 62.7s\n",
            "Epoch 096/120  Loss: 1.3593  Test Acc: 89.28%  Best: 89.48%  LR: 3.11e-04  Time: 63.5s\n",
            "Epoch 097/120  Loss: 1.3560  Test Acc: 89.29%  Best: 89.48%  LR: 2.86e-04  Time: 62.3s\n",
            "Epoch 098/120  Loss: 1.3688  Test Acc: 89.50%  Best: 89.50%  LR: 2.63e-04  Time: 62.7s\n",
            "Epoch 099/120  Loss: 1.3743  Test Acc: 89.86%  Best: 89.86%  LR: 2.40e-04  Time: 63.4s\n",
            "Epoch 100/120  Loss: 1.3360  Test Acc: 89.82%  Best: 89.86%  LR: 2.18e-04  Time: 63.6s\n",
            "Epoch 101/120  Loss: 1.3356  Test Acc: 89.82%  Best: 89.86%  LR: 1.98e-04  Time: 64.4s\n",
            "Epoch 102/120  Loss: 1.3317  Test Acc: 90.07%  Best: 90.07%  LR: 1.78e-04  Time: 62.3s\n",
            "Epoch 103/120  Loss: 1.3571  Test Acc: 90.09%  Best: 90.09%  LR: 1.59e-04  Time: 62.0s\n",
            "Epoch 104/120  Loss: 1.3193  Test Acc: 90.27%  Best: 90.27%  LR: 1.41e-04  Time: 62.4s\n",
            "Epoch 105/120  Loss: 1.3314  Test Acc: 90.34%  Best: 90.34%  LR: 1.24e-04  Time: 64.2s\n",
            "Epoch 106/120  Loss: 1.3231  Test Acc: 90.12%  Best: 90.34%  LR: 1.08e-04  Time: 61.7s\n",
            "Epoch 107/120  Loss: 1.3643  Test Acc: 90.57%  Best: 90.57%  LR: 9.36e-05  Time: 62.1s\n",
            "Epoch 108/120  Loss: 1.3311  Test Acc: 90.40%  Best: 90.57%  LR: 7.99e-05  Time: 62.0s\n",
            "Epoch 109/120  Loss: 1.3401  Test Acc: 90.52%  Best: 90.57%  LR: 6.72e-05  Time: 63.6s\n",
            "Epoch 110/120  Loss: 1.3223  Test Acc: 90.71%  Best: 90.71%  LR: 5.56e-05  Time: 62.7s\n",
            "Epoch 111/120  Loss: 1.3562  Test Acc: 90.63%  Best: 90.71%  LR: 4.51e-05  Time: 61.9s\n",
            "Epoch 112/120  Loss: 1.3135  Test Acc: 90.66%  Best: 90.71%  LR: 3.57e-05  Time: 62.0s\n",
            "Epoch 113/120  Loss: 1.2835  Test Acc: 90.68%  Best: 90.71%  LR: 2.73e-05  Time: 63.2s\n",
            "Epoch 114/120  Loss: 1.3366  Test Acc: 90.75%  Best: 90.75%  LR: 2.01e-05  Time: 62.6s\n",
            "Epoch 115/120  Loss: 1.3475  Test Acc: 90.75%  Best: 90.75%  LR: 1.40e-05  Time: 61.8s\n",
            "Epoch 116/120  Loss: 1.3266  Test Acc: 90.85%  Best: 90.85%  LR: 8.95e-06  Time: 61.5s\n",
            "Epoch 117/120  Loss: 1.3414  Test Acc: 90.92%  Best: 90.92%  LR: 5.03e-06  Time: 63.8s\n",
            "Epoch 118/120  Loss: 1.3340  Test Acc: 90.95%  Best: 90.95%  LR: 2.24e-06  Time: 62.4s\n",
            "Epoch 119/120  Loss: 1.3057  Test Acc: 90.87%  Best: 90.95%  LR: 5.60e-07  Time: 63.2s\n",
            "Epoch 120/120  Loss: 1.3428  Test Acc: 90.87%  Best: 90.95%  LR: 0.00e+00  Time: 61.7s\n",
            "Training finished. Best test acc: 90.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load best and evaluate EMA\n",
        "ckpt = torch.load(\"best_vit_cifar.pth\", map_location=device)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "# optionally use EMA weights for final evaluation:\n",
        "ema_model = ViT(img_size=CFG['img_size'], patch_size=CFG['patch_size'], embed_dim=CFG['embed_dim'],\n",
        "            depth=CFG['depth'], num_heads=CFG['num_heads'], drop_path_rate=CFG['drop_path_rate']).to(device)\n",
        "ema_model.load_state_dict(ckpt['ema'])\n",
        "ema_model.eval()\n",
        "# ema eval\n",
        "correct = 0; total = 0\n",
        "with torch.no_grad():\n",
        "    for images, targets in test_loader:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        outputs = ema_model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "print(\"EMA final Accuracy:\", 100*correct/total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYWPDG3j2BYc",
        "outputId": "fbce8c49-7e22-410e-c34a-ab26bf6f05ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMA final Accuracy: 90.68\n"
          ]
        }
      ]
    }
  ]
}